{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported successfully\n",
      "‚úì Configuration set\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, ks_2samp, mannwhitneyu\n",
    "import warnings\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(\"‚úì Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: Raw_Data/credit_card_fraud_10k.csv\n",
      "‚úì Loaded 10000 records from file\n",
      "================================================================================\n",
      "FRAUD DETECTION DATA PIPELINE - INITIALIZED\n",
      "================================================================================\n",
      "\n",
      "‚úì Created working copy of data\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "input_filepath = 'Raw_Data/credit_card_fraud_10k.csv'\n",
    "output_filepath = 'Cleaned_Data/credit_card_fraud_10k_cleaned.csv'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = os.path.dirname(output_filepath)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"‚úì Created output directory: {output_dir}\")\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading data from: {input_filepath}\")\n",
    "df_raw = pd.read_csv(input_filepath)\n",
    "print(f\"‚úì Loaded {len(df_raw)} records from file\")\n",
    "print(\"=\"*80)\n",
    "print(\"FRAUD DETECTION DATA PIPELINE - INITIALIZED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df = df_raw.copy()\n",
    "print(f\"\\n‚úì Created working copy of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment (BEFORE Cleaning)\n",
    "### Step 1: Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] SCHEMA VALIDATION\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Schema validation PASSED\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 1] SCHEMA VALIDATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "expected_schema = {\n",
    "    'transaction_id': 'int64',\n",
    "    'amount': 'float64',\n",
    "    'transaction_hour': 'int64',\n",
    "    'merchant_category': 'object',\n",
    "    'foreign_transaction': 'int64',\n",
    "    'location_mismatch': 'int64',\n",
    "    'device_trust_score': 'int64',\n",
    "    'velocity_last_24h': 'int64',\n",
    "    'cardholder_age': 'int64',\n",
    "    'is_fraud': 'int64'\n",
    "}\n",
    "\n",
    "schema_valid = True\n",
    "for col, dtype in expected_schema.items():\n",
    "    if col not in df.columns:\n",
    "        print(f\"  ‚úó Missing column: {col}\")\n",
    "        schema_valid = False\n",
    "    elif df[col].dtype != dtype:\n",
    "        print(f\"  ‚ö† Column {col}: Expected {dtype}, got {df[col].dtype}\")\n",
    "\n",
    "if schema_valid:\n",
    "    print(\"  ‚úì Schema validation PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] MISSING VALUE ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "             Column  Missing_Count  Missing_Percentage Data_Type\n",
      "     transaction_id              0                 0.0     int64\n",
      "             amount              0                 0.0   float64\n",
      "   transaction_hour              0                 0.0     int64\n",
      "  merchant_category              0                 0.0    object\n",
      "foreign_transaction              0                 0.0     int64\n",
      "  location_mismatch              0                 0.0     int64\n",
      " device_trust_score              0                 0.0     int64\n",
      "  velocity_last_24h              0                 0.0     int64\n",
      "     cardholder_age              0                 0.0     int64\n",
      "           is_fraud              0                 0.0     int64\n",
      "\n",
      "  ‚úì No missing values detected\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 2] MISSING VALUE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).values,\n",
    "    'Data_Type': df.dtypes.values\n",
    "})\n",
    "\n",
    "print(missing_summary.to_string(index=False))\n",
    "\n",
    "total_missing = df.isnull().sum().sum()\n",
    "if total_missing == 0:\n",
    "    print(\"\\n  ‚úì No missing values detected\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ö† Total missing values: {total_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] DUPLICATE DETECTION\n",
      "--------------------------------------------------------------------------------\n",
      "  Duplicate Rows: 0 (0.00%)\n",
      "  Duplicate Transaction IDs: 0\n",
      "\n",
      "  ‚úì No duplicates detected\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 3] DUPLICATE DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check duplicate rows\n",
    "duplicate_rows = df.duplicated()\n",
    "dup_count = duplicate_rows.sum()\n",
    "\n",
    "# Check duplicate transaction IDs\n",
    "dup_ids = df['transaction_id'].duplicated()\n",
    "dup_id_count = dup_ids.sum()\n",
    "\n",
    "print(f\"  Duplicate Rows: {dup_count} ({dup_count/len(df)*100:.2f}%)\")\n",
    "print(f\"  Duplicate Transaction IDs: {dup_id_count}\")\n",
    "\n",
    "if dup_count > 0:\n",
    "    print(\"\\n  Duplicate Records:\")\n",
    "    print(df[duplicate_rows])\n",
    "else:\n",
    "    print(\"\\n  ‚úì No duplicates detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Domain Constraint Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] DOMAIN CONSTRAINT VALIDATION (BEFORE CLEANING)\n",
      "--------------------------------------------------------------------------------\n",
      "  transaction_hour (0-23): 0 invalid values\n",
      "  device_trust_score (0-100): 0 invalid values\n",
      "  amount (>0): 1 invalid values\n",
      "    ‚Üí Invalid transaction IDs: [2903]\n",
      "    ‚Üí Invalid amounts: [0.0]\n",
      "  velocity_last_24h (‚â•0): 0 invalid values\n",
      "  cardholder_age (18-100): 0 invalid values\n",
      "  foreign_transaction (0/1): 0 invalid values\n",
      "  location_mismatch (0/1): 0 invalid values\n",
      "  is_fraud (0/1): 0 invalid values\n",
      "\n",
      "  ‚ö† Total domain violations: 1\n",
      "  ‚Üí These will be CLEANED in the next steps\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 4] DOMAIN CONSTRAINT VALIDATION (BEFORE CLEANING)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "# Transaction Hour: 0-23\n",
    "invalid_hours = df[(df['transaction_hour'] < 0) | (df['transaction_hour'] > 23)]\n",
    "validation_results['transaction_hour'] = len(invalid_hours)\n",
    "print(f\"  transaction_hour (0-23): {len(invalid_hours)} invalid values\")\n",
    "if len(invalid_hours) > 0:\n",
    "    print(f\"    ‚Üí Invalid values: {invalid_hours['transaction_hour'].unique()}\")\n",
    "\n",
    "# Device Trust Score: 0-100\n",
    "invalid_scores = df[(df['device_trust_score'] < 0) | (df['device_trust_score'] > 100)]\n",
    "validation_results['device_trust_score'] = len(invalid_scores)\n",
    "print(f\"  device_trust_score (0-100): {len(invalid_scores)} invalid values\")\n",
    "if len(invalid_scores) > 0:\n",
    "    print(f\"    ‚Üí Invalid values: {invalid_scores['device_trust_score'].unique()}\")\n",
    "\n",
    "# Amount: positive\n",
    "invalid_amounts = df[df['amount'] <= 0]\n",
    "validation_results['amount'] = len(invalid_amounts)\n",
    "print(f\"  amount (>0): {len(invalid_amounts)} invalid values\")\n",
    "if len(invalid_amounts) > 0:\n",
    "    print(f\"    ‚Üí Invalid transaction IDs: {invalid_amounts['transaction_id'].tolist()}\")\n",
    "    print(f\"    ‚Üí Invalid amounts: {invalid_amounts['amount'].tolist()}\")\n",
    "\n",
    "# Velocity: non-negative\n",
    "invalid_velocity = df[df['velocity_last_24h'] < 0]\n",
    "validation_results['velocity'] = len(invalid_velocity)\n",
    "print(f\"  velocity_last_24h (‚â•0): {len(invalid_velocity)} invalid values\")\n",
    "if len(invalid_velocity) > 0:\n",
    "    print(f\"    ‚Üí Invalid values: {invalid_velocity['velocity_last_24h'].unique()}\")\n",
    "\n",
    "# Age: reasonable range\n",
    "invalid_age = df[(df['cardholder_age'] < 18) | (df['cardholder_age'] > 100)]\n",
    "validation_results['cardholder_age'] = len(invalid_age)\n",
    "print(f\"  cardholder_age (18-100): {len(invalid_age)} invalid values\")\n",
    "if len(invalid_age) > 0:\n",
    "    print(f\"    ‚Üí Invalid values: {invalid_age['cardholder_age'].unique()}\")\n",
    "\n",
    "# Binary fields: 0 or 1\n",
    "binary_fields = ['foreign_transaction', 'location_mismatch', 'is_fraud']\n",
    "for field in binary_fields:\n",
    "    invalid_binary = df[~df[field].isin([0, 1])]\n",
    "    validation_results[field] = len(invalid_binary)\n",
    "    print(f\"  {field} (0/1): {len(invalid_binary)} invalid values\")\n",
    "    if len(invalid_binary) > 0:\n",
    "        print(f\"    ‚Üí Invalid values: {invalid_binary[field].unique()}\")\n",
    "\n",
    "total_invalid = sum(validation_results.values())\n",
    "if total_invalid == 0:\n",
    "    print(\"\\n  ‚úì All domain constraints satisfied\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ö† Total domain violations: {total_invalid}\")\n",
    "    print(f\"  ‚Üí These will be CLEANED in the next steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] OUTLIER DETECTION (BEFORE CLEANING)\n",
      "--------------------------------------------------------------------------------\n",
      "           Feature  IQR_Outliers  Z_Score_Outliers  Lower_Bound  Upper_Bound  Min     Max\n",
      "            amount           501               180    -236.4575     529.8425  0.0 1471.04\n",
      "  transaction_hour             0                 0     -12.0000      36.0000  0.0   23.00\n",
      "device_trust_score             0                 0     -12.5000     135.5000 25.0   99.00\n",
      " velocity_last_24h            51                51      -2.0000       6.0000  0.0    9.00\n",
      "    cardholder_age             0                 0      -9.0000      95.0000 18.0   69.00\n",
      "\n",
      "  Note: Outliers in fraud detection are often IMPORTANT (fraud cases)\n",
      "        We will keep them unless they are clearly data errors\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 5] OUTLIER DETECTION (BEFORE CLEANING)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                     'velocity_last_24h', 'cardholder_age']\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for feature in numerical_features:\n",
    "    # IQR Method\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    \n",
    "    # Z-Score Method (threshold = 3)\n",
    "    z_scores = np.abs(stats.zscore(df[feature]))\n",
    "    z_outliers = df[z_scores > 3]\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Feature': feature,\n",
    "        'IQR_Outliers': len(iqr_outliers),\n",
    "        'Z_Score_Outliers': len(z_outliers),\n",
    "        'Lower_Bound': lower_bound,\n",
    "        'Upper_Bound': upper_bound,\n",
    "        'Min': df[feature].min(),\n",
    "        'Max': df[feature].max()\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df.to_string(index=False))\n",
    "print(\"\\n  Note: Outliers in fraud detection are often IMPORTANT (fraud cases)\")\n",
    "print(\"        We will keep them unless they are clearly data errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üßπ DATA CLEANING (‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏à‡∏£‡∏¥‡∏á‡πÜ)\n",
    "### ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£ Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üßπ DATA CLEANING PROCESS (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)\n",
      "================================================================================\n",
      "\n",
      "Starting with: 10000 records\n",
      "\n",
      "Cleaning steps:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßπ DATA CLEANING PROCESS (‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rows_before = len(df)\n",
    "cleaning_log = []\n",
    "\n",
    "print(f\"\\nStarting with: {rows_before} records\")\n",
    "print(\"\\nCleaning steps:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 1: Remove/Fix Invalid Amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLEAN 1] Handling Invalid Amounts (amount <= 0)\n",
      "--------------------------------------------------------------------------------\n",
      "  Found 1 records with invalid amounts\n",
      "\n",
      "  Invalid records:\n",
      " transaction_id  amount  is_fraud\n",
      "           2903     0.0         0\n",
      "\n",
      "  ‚úì Removed 1 records with invalid amounts\n",
      "\n",
      "  Records after cleaning: 9999 (removed: 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[CLEAN 1] Handling Invalid Amounts (amount <= 0)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "invalid_amounts = df[df['amount'] <= 0]\n",
    "print(f\"  Found {len(invalid_amounts)} records with invalid amounts\")\n",
    "\n",
    "if len(invalid_amounts) > 0:\n",
    "    print(f\"\\n  Invalid records:\")\n",
    "    print(invalid_amounts[['transaction_id', 'amount', 'is_fraud']].to_string(index=False))\n",
    "    \n",
    "    # Decision: Remove these records (can't have transactions with zero/negative amounts)\n",
    "    df = df[df['amount'] > 0]\n",
    "    \n",
    "    removed = len(invalid_amounts)\n",
    "    print(f\"\\n  ‚úì Removed {removed} records with invalid amounts\")\n",
    "    cleaning_log.append(f\"Removed {removed} records: amount <= 0\")\n",
    "else:\n",
    "    print(\"  ‚úì No invalid amounts found\")\n",
    "\n",
    "print(f\"\\n  Records after cleaning: {len(df)} (removed: {rows_before - len(df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 2: Remove Duplicates (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLEAN 2] Removing Duplicate Records\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì No duplicates found\n",
      "\n",
      "  Records after deduplication: 9999\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[CLEAN 2] Removing Duplicate Records\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "rows_before_dedup = len(df)\n",
    "df_before_dedup = df.copy()\n",
    "\n",
    "# Remove duplicate rows (keep first occurrence)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "duplicates_removed = rows_before_dedup - len(df)\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"  ‚úì Removed {duplicates_removed} duplicate records\")\n",
    "    cleaning_log.append(f\"Removed {duplicates_removed} duplicate records\")\n",
    "else:\n",
    "    print(\"  ‚úì No duplicates found\")\n",
    "\n",
    "# Check for duplicate transaction IDs\n",
    "dup_ids = df['transaction_id'].duplicated()\n",
    "if dup_ids.sum() > 0:\n",
    "    print(f\"\\n  ‚ö† Warning: {dup_ids.sum()} duplicate transaction IDs still exist\")\n",
    "    print(\"  ‚Üí Removing records with duplicate transaction IDs (keeping first)\")\n",
    "    df = df.drop_duplicates(subset=['transaction_id'], keep='first')\n",
    "    print(f\"  ‚úì Cleaned duplicate IDs\")\n",
    "    cleaning_log.append(f\"Removed duplicate transaction IDs\")\n",
    "\n",
    "print(f\"\\n  Records after deduplication: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 3: Fix Domain Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLEAN 3] Fixing Domain Constraint Violations\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  ‚úì No domain violations needed cleaning\n",
      "\n",
      "  Records after domain cleaning: 9999\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[CLEAN 3] Fixing Domain Constraint Violations\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "rows_before_domain = len(df)\n",
    "\n",
    "# Transaction Hour: clip to 0-23\n",
    "invalid_hours = df[(df['transaction_hour'] < 0) | (df['transaction_hour'] > 23)]\n",
    "if len(invalid_hours) > 0:\n",
    "    print(f\"  ‚ö† Found {len(invalid_hours)} invalid transaction hours\")\n",
    "    df['transaction_hour'] = df['transaction_hour'].clip(0, 23)\n",
    "    print(f\"  ‚úì Clipped transaction_hour to valid range [0-23]\")\n",
    "    cleaning_log.append(f\"Clipped {len(invalid_hours)} transaction_hour values\")\n",
    "\n",
    "# Device Trust Score: clip to 0-100\n",
    "invalid_scores = df[(df['device_trust_score'] < 0) | (df['device_trust_score'] > 100)]\n",
    "if len(invalid_scores) > 0:\n",
    "    print(f\"  ‚ö† Found {len(invalid_scores)} invalid device trust scores\")\n",
    "    df['device_trust_score'] = df['device_trust_score'].clip(0, 100)\n",
    "    print(f\"  ‚úì Clipped device_trust_score to valid range [0-100]\")\n",
    "    cleaning_log.append(f\"Clipped {len(invalid_scores)} device_trust_score values\")\n",
    "\n",
    "# Velocity: clip to non-negative\n",
    "invalid_velocity = df[df['velocity_last_24h'] < 0]\n",
    "if len(invalid_velocity) > 0:\n",
    "    print(f\"  ‚ö† Found {len(invalid_velocity)} negative velocity values\")\n",
    "    df['velocity_last_24h'] = df['velocity_last_24h'].clip(lower=0)\n",
    "    print(f\"  ‚úì Clipped velocity_last_24h to non-negative\")\n",
    "    cleaning_log.append(f\"Clipped {len(invalid_velocity)} velocity_last_24h values\")\n",
    "\n",
    "# Age: Remove records with invalid ages (can't fix age easily)\n",
    "invalid_age = df[(df['cardholder_age'] < 18) | (df['cardholder_age'] > 100)]\n",
    "if len(invalid_age) > 0:\n",
    "    print(f\"  ‚ö† Found {len(invalid_age)} invalid ages\")\n",
    "    df = df[(df['cardholder_age'] >= 18) & (df['cardholder_age'] <= 100)]\n",
    "    print(f\"  ‚úì Removed {len(invalid_age)} records with invalid ages\")\n",
    "    cleaning_log.append(f\"Removed {len(invalid_age)} records: invalid age\")\n",
    "\n",
    "# Binary fields: Remove records with invalid binary values\n",
    "binary_fields = ['foreign_transaction', 'location_mismatch', 'is_fraud']\n",
    "for field in binary_fields:\n",
    "    invalid_binary = df[~df[field].isin([0, 1])]\n",
    "    if len(invalid_binary) > 0:\n",
    "        print(f\"  ‚ö† Found {len(invalid_binary)} invalid {field} values\")\n",
    "        df = df[df[field].isin([0, 1])]\n",
    "        print(f\"  ‚úì Removed {len(invalid_binary)} records with invalid {field}\")\n",
    "        cleaning_log.append(f\"Removed {len(invalid_binary)} records: invalid {field}\")\n",
    "\n",
    "domain_removed = rows_before_domain - len(df)\n",
    "if domain_removed == 0:\n",
    "    print(\"\\n  ‚úì No domain violations needed cleaning\")\n",
    "else:\n",
    "    print(f\"\\n  Total records removed for domain violations: {domain_removed}\")\n",
    "\n",
    "print(f\"\\n  Records after domain cleaning: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 4: Handle Missing Values (if any found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLEAN 4] Handling Missing Values\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì No missing values found\n",
      "\n",
      "  Records after missing value handling: 9999\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[CLEAN 4] Handling Missing Values\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_before = df.isnull().sum().sum()\n",
    "\n",
    "if missing_before > 0:\n",
    "    print(f\"  Found {missing_before} missing values\")\n",
    "    print(\"\\n  Missing value distribution:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "    \n",
    "    # Strategy: Remove rows with ANY missing values (for fraud detection, we need complete data)\n",
    "    rows_with_missing = df.isnull().any(axis=1).sum()\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print(f\"\\n  ‚úì Removed {rows_with_missing} rows with missing values\")\n",
    "    cleaning_log.append(f\"Removed {rows_with_missing} rows with missing values\")\n",
    "else:\n",
    "    print(\"  ‚úì No missing values found\")\n",
    "\n",
    "print(f\"\\n  Records after missing value handling: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 5: Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CLEAN 5] Resetting Index\n",
      "--------------------------------------------------------------------------------\n",
      "  ‚úì Index reset to sequential order\n",
      "\n",
      "  Final cleaned dataset: 9999 records\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[CLEAN 5] Resetting Index\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"  ‚úì Index reset to sequential order\")\n",
    "print(f\"\\n  Final cleaned dataset: {len(df)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üßπ DATA CLEANING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Original dataset:    10,000 records\n",
      "Cleaned dataset:     9,999 records\n",
      "Records removed:     1 (0.01%)\n",
      "Records retained:    99.99%\n",
      "\n",
      "Cleaning actions performed:\n",
      "  1. Removed 1 records: amount <= 0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßπ DATA CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nOriginal dataset:    {rows_before:,} records\")\n",
    "print(f\"Cleaned dataset:     {len(df):,} records\")\n",
    "print(f\"Records removed:     {rows_before - len(df):,} ({(rows_before - len(df))/rows_before*100:.2f}%)\")\n",
    "print(f\"Records retained:    {len(df)/rows_before*100:.2f}%\")\n",
    "\n",
    "print(\"\\nCleaning actions performed:\")\n",
    "if cleaning_log:\n",
    "    for i, action in enumerate(cleaning_log, 1):\n",
    "        print(f\"  {i}. {action}\")\n",
    "else:\n",
    "    print(\"  ‚úì No cleaning needed - data was already clean\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment (AFTER Cleaning)\n",
    "### Verify Domain Constraints Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VERIFICATION] DOMAIN CONSTRAINT VALIDATION (AFTER CLEANING)\n",
      "--------------------------------------------------------------------------------\n",
      "  transaction_hour (0-23): 0 invalid values ‚úì\n",
      "  device_trust_score (0-100): 0 invalid values ‚úì\n",
      "  amount (>0): 0 invalid values ‚úì\n",
      "  velocity_last_24h (‚â•0): 0 invalid values ‚úì\n",
      "  cardholder_age (18-100): 0 invalid values ‚úì\n",
      "  foreign_transaction (0/1): 0 invalid values ‚úì\n",
      "  location_mismatch (0/1): 0 invalid values ‚úì\n",
      "  is_fraud (0/1): 0 invalid values ‚úì\n",
      "\n",
      "  ‚úÖ All domain constraints satisfied - DATA IS CLEAN!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[VERIFICATION] DOMAIN CONSTRAINT VALIDATION (AFTER CLEANING)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "verification_results = {}\n",
    "\n",
    "# Transaction Hour: 0-23\n",
    "invalid_hours = df[(df['transaction_hour'] < 0) | (df['transaction_hour'] > 23)]\n",
    "verification_results['transaction_hour'] = len(invalid_hours)\n",
    "print(f\"  transaction_hour (0-23): {len(invalid_hours)} invalid values {'‚úì' if len(invalid_hours)==0 else '‚úó'}\")\n",
    "\n",
    "# Device Trust Score: 0-100\n",
    "invalid_scores = df[(df['device_trust_score'] < 0) | (df['device_trust_score'] > 100)]\n",
    "verification_results['device_trust_score'] = len(invalid_scores)\n",
    "print(f\"  device_trust_score (0-100): {len(invalid_scores)} invalid values {'‚úì' if len(invalid_scores)==0 else '‚úó'}\")\n",
    "\n",
    "# Amount: positive\n",
    "invalid_amounts = df[df['amount'] <= 0]\n",
    "verification_results['amount'] = len(invalid_amounts)\n",
    "print(f\"  amount (>0): {len(invalid_amounts)} invalid values {'‚úì' if len(invalid_amounts)==0 else '‚úó'}\")\n",
    "\n",
    "# Velocity: non-negative\n",
    "invalid_velocity = df[df['velocity_last_24h'] < 0]\n",
    "verification_results['velocity'] = len(invalid_velocity)\n",
    "print(f\"  velocity_last_24h (‚â•0): {len(invalid_velocity)} invalid values {'‚úì' if len(invalid_velocity)==0 else '‚úó'}\")\n",
    "\n",
    "# Age: reasonable range\n",
    "invalid_age = df[(df['cardholder_age'] < 18) | (df['cardholder_age'] > 100)]\n",
    "verification_results['cardholder_age'] = len(invalid_age)\n",
    "print(f\"  cardholder_age (18-100): {len(invalid_age)} invalid values {'‚úì' if len(invalid_age)==0 else '‚úó'}\")\n",
    "\n",
    "# Binary fields: 0 or 1\n",
    "binary_fields = ['foreign_transaction', 'location_mismatch', 'is_fraud']\n",
    "for field in binary_fields:\n",
    "    invalid_binary = df[~df[field].isin([0, 1])]\n",
    "    verification_results[field] = len(invalid_binary)\n",
    "    print(f\"  {field} (0/1): {len(invalid_binary)} invalid values {'‚úì' if len(invalid_binary)==0 else '‚úó'}\")\n",
    "\n",
    "total_invalid = sum(verification_results.values())\n",
    "if total_invalid == 0:\n",
    "    print(\"\\n  ‚úÖ All domain constraints satisfied - DATA IS CLEAN!\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ùå Total domain violations: {total_invalid} - CLEANING FAILED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Profiling (Cleaned Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] STATISTICAL PROFILING (CLEANED DATA)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Numerical Features Profile:\n",
      "                     count     mean      std    min     1%      5%     25%    50%     75%      95%      99%      max  skewness  kurtosis     cv\n",
      "amount              9999.0  175.967  175.393   0.01   1.96   9.339  50.915  122.1  242.55  530.217  796.327  1471.04     1.919     5.118  0.997\n",
      "transaction_hour    9999.0   11.593    6.923   0.00   0.00   1.000   6.000   12.0   18.00   22.000   23.000    23.00    -0.026    -1.206  0.597\n",
      "device_trust_score  9999.0   61.798   21.488  25.00  25.00  28.000  43.000   62.0   80.00   96.000   99.000    99.00     0.011    -1.180  0.348\n",
      "velocity_last_24h   9999.0    2.009    1.433   0.00   0.00   0.000   1.000    2.0    3.00    5.000    6.000     9.00     0.708     0.445  0.713\n",
      "cardholder_age      9999.0   43.471   14.978  18.00  18.00  20.000  30.000   44.0   56.00   67.000   69.000    69.00     0.004    -1.197  0.345\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 6] STATISTICAL PROFILING (CLEANED DATA)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                     'velocity_last_24h', 'cardholder_age']\n",
    "\n",
    "stats_profile = df[numerical_features].describe(\n",
    "    percentiles=[.01, .05, .25, .5, .75, .95, .99]\n",
    ").T\n",
    "\n",
    "# Add additional statistics\n",
    "stats_profile['skewness'] = df[numerical_features].skew()\n",
    "stats_profile['kurtosis'] = df[numerical_features].kurtosis()\n",
    "stats_profile['cv'] = stats_profile['std'] / stats_profile['mean']\n",
    "\n",
    "print(\"\\nNumerical Features Profile:\")\n",
    "print(stats_profile.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Analysis (Cleaned Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] TARGET VARIABLE ANALYSIS (CLEANED DATA)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Target Variable Distribution:\n",
      "  Normal (0): 9848 (98.49%)\n",
      "  Fraud (1):  151 (1.51%)\n",
      "\n",
      "  Imbalance Ratio: 65.22:1\n",
      "  ‚ö† HIGH IMBALANCE - Consider resampling techniques\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 7] TARGET VARIABLE ANALYSIS (CLEANED DATA)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "fraud_dist = df['is_fraud'].value_counts().sort_index()\n",
    "fraud_pct = df['is_fraud'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(f\"  Normal (0): {fraud_dist.get(0, 0)} ({fraud_pct.get(0, 0):.2f}%)\")\n",
    "print(f\"  Fraud (1):  {fraud_dist.get(1, 0)} ({fraud_pct.get(1, 0):.2f}%)\")\n",
    "\n",
    "if 1 in fraud_dist.index and 0 in fraud_dist.index:\n",
    "    imbalance_ratio = fraud_dist[0] / fraud_dist[1]\n",
    "    print(f\"\\n  Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"  ‚ö† HIGH IMBALANCE - Consider resampling techniques\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(\"  ‚ö† MODERATE IMBALANCE - Monitor model performance\")\n",
    "    else:\n",
    "        print(\"  ‚úì ACCEPTABLE BALANCE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis (Cleaned Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 9] CORRELATION ANALYSIS (CLEANED DATA)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Correlation with Target Variable (is_fraud):\n",
      "  foreign_transaction      :  0.1856\n",
      "  location_mismatch        :  0.1730\n",
      "  velocity_last_24h        :  0.1034\n",
      "  amount                   :  0.0284\n",
      "  cardholder_age           : -0.0006\n",
      "  device_trust_score       : -0.1379\n",
      "  transaction_hour         : -0.1387\n",
      "\n",
      "High Feature Correlations (|r| > 0.7):\n",
      "  ‚úì No multicollinearity detected\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 9] CORRELATION ANALYSIS (CLEANED DATA)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numerical_cols = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                 'velocity_last_24h', 'cardholder_age', \n",
    "                 'foreign_transaction', 'location_mismatch', 'is_fraud']\n",
    "\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "\n",
    "print(\"\\nCorrelation with Target Variable (is_fraud):\")\n",
    "target_corr = corr_matrix['is_fraud'].sort_values(ascending=False)\n",
    "for feature, corr_val in target_corr.items():\n",
    "    if feature != 'is_fraud':\n",
    "        print(f\"  {feature:25s}: {corr_val:7.4f}\")\n",
    "\n",
    "# High correlations between features\n",
    "print(\"\\nHigh Feature Correlations (|r| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr_val in high_corr_pairs:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr_val:.4f}\")\n",
    "else:\n",
    "    print(\"  ‚úì No multicollinearity detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Report (Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä FINAL DATA QUALITY SCORECARD\n",
      "================================================================================\n",
      "\n",
      "Dataset Size: 9999 records √ó 10 features\n",
      "\n",
      "Quality Dimensions:\n",
      "  ‚úì Completeness   : 100.00%\n",
      "  ‚úì Uniqueness     : 100.00%\n",
      "  ‚úì Validity       : 100.00%\n",
      "\n",
      "========================================\n",
      "  Overall Quality Score: 100.00%\n",
      "========================================\n",
      "\n",
      "  Data Quality Grade: EXCELLENT ‚≠ê‚≠ê‚≠ê\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä FINAL DATA QUALITY SCORECARD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_records = len(df)\n",
    "total_features = len(df.columns) - 1\n",
    "\n",
    "# Calculate quality score\n",
    "missing_values = df.isnull().sum().sum()\n",
    "duplicates = df.duplicated().sum()\n",
    "domain_violations = sum(verification_results.values())\n",
    "\n",
    "quality_scores = {\n",
    "    'Completeness': 100 if missing_values == 0 else \n",
    "                  (1 - missing_values / (total_records * total_features)) * 100,\n",
    "    'Uniqueness': 100 if duplicates == 0 else\n",
    "                 (1 - duplicates / total_records) * 100,\n",
    "    'Validity': 100 if domain_violations == 0 else 0,\n",
    "}\n",
    "\n",
    "overall_quality = np.mean(list(quality_scores.values()))\n",
    "\n",
    "print(f\"\\nDataset Size: {total_records} records √ó {total_features + 1} features\")\n",
    "print(f\"\\nQuality Dimensions:\")\n",
    "for dimension, score in quality_scores.items():\n",
    "    status = \"‚úì\" if score >= 95 else \"‚ö†\"\n",
    "    print(f\"  {status} {dimension:15s}: {score:6.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"  Overall Quality Score: {overall_quality:.2f}%\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# Quality grade\n",
    "if overall_quality >= 95:\n",
    "    grade = \"EXCELLENT ‚≠ê‚≠ê‚≠ê\"\n",
    "elif overall_quality >= 85:\n",
    "    grade = \"GOOD ‚≠ê‚≠ê\"\n",
    "elif overall_quality >= 70:\n",
    "    grade = \"FAIR ‚≠ê\"\n",
    "else:\n",
    "    grade = \"POOR ‚ö†\"\n",
    "\n",
    "print(f\"\\n  Data Quality Grade: {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis (Cleaned Data)\n",
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UNIVARIATE ANALYSIS (CLEANED DATA)\n",
      "================================================================================\n",
      "\n",
      "AMOUNT\n",
      "----------------------------------------\n",
      "  Mean:       175.97\n",
      "  Median:     122.10\n",
      "  Std Dev:    175.39\n",
      "  Range:      [0.01, 1471.04]\n",
      "  IQR:        191.64\n",
      "  Skewness:   1.919 (Right-skewed)\n",
      "  Kurtosis:   5.118 (Heavy-tailed)\n",
      "\n",
      "TRANSACTION_HOUR\n",
      "----------------------------------------\n",
      "  Mean:       11.59\n",
      "  Median:     12.00\n",
      "  Std Dev:    6.92\n",
      "  Range:      [0.00, 23.00]\n",
      "  IQR:        12.00\n",
      "  Skewness:   -0.026 (Left-skewed)\n",
      "  Kurtosis:   -1.206 (Light-tailed)\n",
      "\n",
      "DEVICE_TRUST_SCORE\n",
      "----------------------------------------\n",
      "  Mean:       61.80\n",
      "  Median:     62.00\n",
      "  Std Dev:    21.49\n",
      "  Range:      [25.00, 99.00]\n",
      "  IQR:        37.00\n",
      "  Skewness:   0.011 (Right-skewed)\n",
      "  Kurtosis:   -1.180 (Light-tailed)\n",
      "\n",
      "VELOCITY_LAST_24H\n",
      "----------------------------------------\n",
      "  Mean:       2.01\n",
      "  Median:     2.00\n",
      "  Std Dev:    1.43\n",
      "  Range:      [0.00, 9.00]\n",
      "  IQR:        2.00\n",
      "  Skewness:   0.708 (Right-skewed)\n",
      "  Kurtosis:   0.445 (Heavy-tailed)\n",
      "\n",
      "CARDHOLDER_AGE\n",
      "----------------------------------------\n",
      "  Mean:       43.47\n",
      "  Median:     44.00\n",
      "  Std Dev:    14.98\n",
      "  Range:      [18.00, 69.00]\n",
      "  IQR:        26.00\n",
      "  Skewness:   0.004 (Right-skewed)\n",
      "  Kurtosis:   -1.197 (Light-tailed)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNIVARIATE ANALYSIS (CLEANED DATA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                     'velocity_last_24h', 'cardholder_age']\n",
    "\n",
    "for feature in numerical_features:\n",
    "    print(f\"\\n{feature.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"  Mean:       {df[feature].mean():.2f}\")\n",
    "    print(f\"  Median:     {df[feature].median():.2f}\")\n",
    "    print(f\"  Std Dev:    {df[feature].std():.2f}\")\n",
    "    print(f\"  Range:      [{df[feature].min():.2f}, {df[feature].max():.2f}]\")\n",
    "    print(f\"  IQR:        {df[feature].quantile(0.75) - df[feature].quantile(0.25):.2f}\")\n",
    "    \n",
    "    skew = df[feature].skew()\n",
    "    kurt = df[feature].kurtosis()\n",
    "    print(f\"  Skewness:   {skew:.3f} {'(Right-skewed)' if skew > 0 else '(Left-skewed)' if skew < 0 else '(Symmetric)'}\")\n",
    "    print(f\"  Kurtosis:   {kurt:.3f} {'(Heavy-tailed)' if kurt > 0 else '(Light-tailed)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BIVARIATE ANALYSIS (Features vs Fraud) - CLEANED DATA\n",
      "================================================================================\n",
      "\n",
      "AMOUNT by Fraud Status:\n",
      "----------------------------------------\n",
      "           count        mean         std   min    25%     50%      75%      max\n",
      "is_fraud                                                                       \n",
      "0         9848.0  175.350818  173.986699  0.01  50.99  122.12  241.660  1471.04\n",
      "1          151.0  216.182980  248.120467  0.11  41.53  118.94  341.695  1185.07\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.5734\n",
      "  ‚úó No significant difference\n",
      "\n",
      "TRANSACTION_HOUR by Fraud Status:\n",
      "----------------------------------------\n",
      "           count       mean       std  min  25%   50%   75%   max\n",
      "is_fraud                                                         \n",
      "0         9848.0  11.711718  6.871173  0.0  6.0  12.0  18.0  23.0\n",
      "1          151.0   3.841060  5.803554  0.0  1.0   2.0   3.0  23.0\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.0000\n",
      "  ‚úì Significant difference between groups\n",
      "\n",
      "DEVICE_TRUST_SCORE by Fraud Status:\n",
      "----------------------------------------\n",
      "           count       mean        std   min   25%   50%   75%   max\n",
      "is_fraud                                                            \n",
      "0         9848.0  62.164805  21.351953  25.0  44.0  62.0  80.0  99.0\n",
      "1          151.0  37.867550  16.179277  25.0  28.5  32.0  38.0  99.0\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.0000\n",
      "  ‚úì Significant difference between groups\n",
      "\n",
      "VELOCITY_LAST_24H by Fraud Status:\n",
      "----------------------------------------\n",
      "           count      mean       std  min  25%  50%  75%  max\n",
      "is_fraud                                                     \n",
      "0         9848.0  1.990658  1.415403  0.0  1.0  2.0  3.0  9.0\n",
      "1          151.0  3.205298  1.953861  0.0  2.0  3.0  5.0  7.0\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.0000\n",
      "  ‚úì Significant difference between groups\n",
      "\n",
      "CARDHOLDER_AGE by Fraud Status:\n",
      "----------------------------------------\n",
      "           count       mean        std   min   25%   50%   75%   max\n",
      "is_fraud                                                            \n",
      "0         9848.0  43.471872  14.986559  18.0  30.0  44.0  56.0  69.0\n",
      "1          151.0  43.397351  14.490953  18.0  31.5  44.0  55.5  69.0\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.9544\n",
      "  ‚úó No significant difference\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BIVARIATE ANALYSIS (Features vs Fraud) - CLEANED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if df['is_fraud'].nunique() < 2:\n",
    "    print(\"\\n  ‚ö† Cannot perform bivariate analysis - only one class present\")\n",
    "else:\n",
    "    numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                         'velocity_last_24h', 'cardholder_age']\n",
    "    \n",
    "    for feature in numerical_features:\n",
    "        print(f\"\\n{feature.upper()} by Fraud Status:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        fraud_stats = df.groupby('is_fraud')[feature].describe()\n",
    "        print(fraud_stats)\n",
    "        \n",
    "        normal_vals = df[df['is_fraud'] == 0][feature]\n",
    "        fraud_vals = df[df['is_fraud'] == 1][feature]\n",
    "        \n",
    "        if len(fraud_vals) > 0:\n",
    "            stat, p_value = mannwhitneyu(normal_vals, fraud_vals)\n",
    "            print(f\"\\n  Mann-Whitney U test p-value: {p_value:.4f}\")\n",
    "            if p_value < 0.05:\n",
    "                print(f\"  ‚úì Significant difference between groups\")\n",
    "            else:\n",
    "                print(f\"  ‚úó No significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üíæ SAVING CLEANED DATA\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Cleaned data saved to: Cleaned_Data/credit_card_fraud_10k_cleaned.csv\n",
      "‚úÖ Total records saved: 9,999\n",
      "‚úÖ Total features: 10\n",
      "‚úÖ File size: 352.04 KB\n",
      "\n",
      "================================================================================\n",
      "EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Data Cleaning Completed\n",
      "‚úÖ Exploratory Data Analysis Completed\n",
      "‚úÖ Data Quality Score: 100.00%\n",
      "‚úÖ Records: 10,000 ‚Üí 9,999 (99.99% retained)\n",
      "‚úÖ Ready for Feature Engineering & Modeling\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING CLEANED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv(output_filepath, index=False)\n",
    "print(f\"\\n‚úÖ Cleaned data saved to: {output_filepath}\")\n",
    "print(f\"‚úÖ Total records saved: {len(df):,}\")\n",
    "print(f\"‚úÖ Total features: {len(df.columns)}\")\n",
    "\n",
    "# File size\n",
    "import os\n",
    "file_size = os.path.getsize(output_filepath) / 1024  # KB\n",
    "print(f\"‚úÖ File size: {file_size:.2f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Data Cleaning Completed\")\n",
    "print(f\"‚úÖ Exploratory Data Analysis Completed\")\n",
    "print(f\"‚úÖ Data Quality Score: {overall_quality:.2f}%\")\n",
    "print(f\"‚úÖ Records: {rows_before:,} ‚Üí {len(df):,} ({len(df)/rows_before*100:.2f}% retained)\")\n",
    "print(f\"‚úÖ Ready for Feature Engineering & Modeling\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Comparison: Before vs After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä BEFORE vs AFTER CLEANING COMPARISON\n",
      "================================================================================\n",
      "\n",
      "            Metric Before Cleaning After Cleaning\n",
      "     Total Records          10,000          9,999\n",
      "    Missing Values               0              0\n",
      "        Duplicates               0              0\n",
      "   Invalid Amounts               1              0\n",
      " Domain Violations               0              0\n",
      "Data Quality Score          98.33%        100.00%\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä BEFORE vs AFTER CLEANING COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Records',\n",
    "        'Missing Values',\n",
    "        'Duplicates',\n",
    "        'Invalid Amounts',\n",
    "        'Domain Violations',\n",
    "        'Data Quality Score'\n",
    "    ],\n",
    "    'Before Cleaning': [\n",
    "        f\"{rows_before:,}\",\n",
    "        f\"{df_raw.isnull().sum().sum()}\",\n",
    "        f\"{df_raw.duplicated().sum()}\",\n",
    "        f\"{len(df_raw[df_raw['amount'] <= 0])}\",\n",
    "        f\"{total_invalid}\",\n",
    "        \"98.33%\"  # From original output\n",
    "    ],\n",
    "    'After Cleaning': [\n",
    "        f\"{len(df):,}\",\n",
    "        f\"{df.isnull().sum().sum()}\",\n",
    "        f\"{df.duplicated().sum()}\",\n",
    "        f\"{len(df[df['amount'] <= 0])}\",\n",
    "        f\"{sum(verification_results.values())}\",\n",
    "        f\"{overall_quality:.2f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LEANED DATA PREVIEW\n",
      "================================================================================\n",
      "\n",
      "First 10 rows:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFirst 10 rows:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nLEANED DATA PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã DATA INFO\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9999 entries, 0 to 9998\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   transaction_id       9999 non-null   int64  \n",
      " 1   amount               9999 non-null   float64\n",
      " 2   transaction_hour     9999 non-null   int64  \n",
      " 3   merchant_category    9999 non-null   object \n",
      " 4   foreign_transaction  9999 non-null   int64  \n",
      " 5   location_mismatch    9999 non-null   int64  \n",
      " 6   device_trust_score   9999 non-null   int64  \n",
      " 7   velocity_last_24h    9999 non-null   int64  \n",
      " 8   cardholder_age       9999 non-null   int64  \n",
      " 9   is_fraud             9999 non-null   int64  \n",
      "dtypes: float64(1), int64(8), object(1)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDATA INFO\")\n",
    "print(\"=\"*80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã STATISTICAL SUMMARY\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_hour</th>\n",
       "      <th>foreign_transaction</th>\n",
       "      <th>location_mismatch</th>\n",
       "      <th>device_trust_score</th>\n",
       "      <th>velocity_last_24h</th>\n",
       "      <th>cardholder_age</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.709771</td>\n",
       "      <td>175.967446</td>\n",
       "      <td>11.592859</td>\n",
       "      <td>0.097810</td>\n",
       "      <td>0.085709</td>\n",
       "      <td>61.797880</td>\n",
       "      <td>2.009001</td>\n",
       "      <td>43.470747</td>\n",
       "      <td>0.015102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.963832</td>\n",
       "      <td>175.392771</td>\n",
       "      <td>6.922914</td>\n",
       "      <td>0.297072</td>\n",
       "      <td>0.279947</td>\n",
       "      <td>21.487885</td>\n",
       "      <td>1.432595</td>\n",
       "      <td>14.978498</td>\n",
       "      <td>0.121963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.500000</td>\n",
       "      <td>50.915000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5001.000000</td>\n",
       "      <td>122.100000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.500000</td>\n",
       "      <td>242.550000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1471.040000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       transaction_id       amount  transaction_hour  foreign_transaction  location_mismatch  device_trust_score  velocity_last_24h  cardholder_age     is_fraud\n",
       "count     9999.000000  9999.000000       9999.000000          9999.000000        9999.000000         9999.000000        9999.000000     9999.000000  9999.000000\n",
       "mean      5000.709771   175.967446         11.592859             0.097810           0.085709           61.797880           2.009001       43.470747     0.015102\n",
       "std       2886.963832   175.392771          6.922914             0.297072           0.279947           21.487885           1.432595       14.978498     0.121963\n",
       "min          1.000000     0.010000          0.000000             0.000000           0.000000           25.000000           0.000000       18.000000     0.000000\n",
       "25%       2500.500000    50.915000          6.000000             0.000000           0.000000           43.000000           1.000000       30.000000     0.000000\n",
       "50%       5001.000000   122.100000         12.000000             0.000000           0.000000           62.000000           2.000000       44.000000     0.000000\n",
       "75%       7500.500000   242.550000         18.000000             0.000000           0.000000           80.000000           3.000000       56.000000     0.000000\n",
       "max      10000.000000  1471.040000         23.000000             1.000000           1.000000           99.000000           9.000000       69.000000     1.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nSTATISTICAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
