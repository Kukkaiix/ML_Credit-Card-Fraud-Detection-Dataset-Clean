{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Detection Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n",
      "✓ Configuration set\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, ks_2samp, mannwhitneyu\n",
    "import warnings\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(\"✓ Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: Raw_Data/credit_card_fraud_10k.csv\n",
      "✓ Loaded 10000 records from file\n",
      "================================================================================\n",
      "FRAUD DETECTION DATA PIPELINE - INITIALIZED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "input_filepath = 'Raw_Data/credit_card_fraud_10k.csv'\n",
    "output_filepath = 'Cleaned_Data/credit_card_fraud_10k_cleaned.csv'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = os.path.dirname(output_filepath)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"✓ Created output directory: {output_dir}\")\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading data from: {input_filepath}\")\n",
    "df = pd.read_csv(input_filepath)\n",
    "print(f\"✓ Loaded {len(df)} records from file\")\n",
    "print(\"=\"*80)\n",
    "print(\"FRAUD DETECTION DATA PIPELINE - INITIALIZED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "### Step 1: Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] SCHEMA VALIDATION\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Schema validation PASSED\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 1] SCHEMA VALIDATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "expected_schema = {\n",
    "    'transaction_id': 'int64',\n",
    "    'amount': 'float64',\n",
    "    'transaction_hour': 'int64',\n",
    "    'merchant_category': 'object',\n",
    "    'foreign_transaction': 'int64',\n",
    "    'location_mismatch': 'int64',\n",
    "    'device_trust_score': 'int64',\n",
    "    'velocity_last_24h': 'int64',\n",
    "    'cardholder_age': 'int64',\n",
    "    'is_fraud': 'int64'\n",
    "}\n",
    "\n",
    "schema_valid = True\n",
    "for col, dtype in expected_schema.items():\n",
    "    if col not in df.columns:\n",
    "        print(f\"  ✗ Missing column: {col}\")\n",
    "        schema_valid = False\n",
    "    elif df[col].dtype != dtype:\n",
    "        print(f\"  ⚠ Column {col}: Expected {dtype}, got {df[col].dtype}\")\n",
    "\n",
    "if schema_valid:\n",
    "    print(\"  ✓ Schema validation PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] MISSING VALUE ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "             Column  Missing_Count  Missing_Percentage Data_Type\n",
      "     transaction_id              0                 0.0     int64\n",
      "             amount              0                 0.0   float64\n",
      "   transaction_hour              0                 0.0     int64\n",
      "  merchant_category              0                 0.0    object\n",
      "foreign_transaction              0                 0.0     int64\n",
      "  location_mismatch              0                 0.0     int64\n",
      " device_trust_score              0                 0.0     int64\n",
      "  velocity_last_24h              0                 0.0     int64\n",
      "     cardholder_age              0                 0.0     int64\n",
      "           is_fraud              0                 0.0     int64\n",
      "\n",
      "  ✓ No missing values detected\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 2] MISSING VALUE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum().values,\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).values,\n",
    "    'Data_Type': df.dtypes.values\n",
    "})\n",
    "\n",
    "print(missing_summary.to_string(index=False))\n",
    "\n",
    "total_missing = df.isnull().sum().sum()\n",
    "if total_missing == 0:\n",
    "    print(\"\\n  ✓ No missing values detected\")\n",
    "else:\n",
    "    print(f\"\\n  ⚠ Total missing values: {total_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Duplicate Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] DUPLICATE DETECTION\n",
      "--------------------------------------------------------------------------------\n",
      "  Duplicate Rows: 0 (0.00%)\n",
      "  Duplicate Transaction IDs: 0\n",
      "\n",
      "  ✓ No duplicates detected\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 3] DUPLICATE DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check duplicate rows\n",
    "duplicate_rows = df.duplicated()\n",
    "dup_count = duplicate_rows.sum()\n",
    "\n",
    "# Check duplicate transaction IDs\n",
    "dup_ids = df['transaction_id'].duplicated()\n",
    "dup_id_count = dup_ids.sum()\n",
    "\n",
    "print(f\"  Duplicate Rows: {dup_count} ({dup_count/len(df)*100:.2f}%)\")\n",
    "print(f\"  Duplicate Transaction IDs: {dup_id_count}\")\n",
    "\n",
    "if dup_count > 0:\n",
    "    print(\"\\n  Duplicate Records:\")\n",
    "    print(df[duplicate_rows])\n",
    "else:\n",
    "    print(\"\\n  ✓ No duplicates detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Domain Constraint Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] DOMAIN CONSTRAINT VALIDATION\n",
      "--------------------------------------------------------------------------------\n",
      "  transaction_hour (0-23): 0 invalid values\n",
      "  device_trust_score (0-100): 0 invalid values\n",
      "  amount (>0): 1 invalid values\n",
      "  velocity_last_24h (≥0): 0 invalid values\n",
      "  cardholder_age (18-100): 0 invalid values\n",
      "  foreign_transaction (0/1): 0 invalid values\n",
      "  location_mismatch (0/1): 0 invalid values\n",
      "  is_fraud (0/1): 0 invalid values\n",
      "\n",
      "  ⚠ Total domain violations: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 4] DOMAIN CONSTRAINT VALIDATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "# Transaction Hour: 0-23\n",
    "invalid_hours = df[(df['transaction_hour'] < 0) | (df['transaction_hour'] > 23)]\n",
    "validation_results['transaction_hour'] = len(invalid_hours)\n",
    "print(f\"  transaction_hour (0-23): {len(invalid_hours)} invalid values\")\n",
    "\n",
    "# Device Trust Score: 0-100\n",
    "invalid_scores = df[(df['device_trust_score'] < 0) | (df['device_trust_score'] > 100)]\n",
    "validation_results['device_trust_score'] = len(invalid_scores)\n",
    "print(f\"  device_trust_score (0-100): {len(invalid_scores)} invalid values\")\n",
    "\n",
    "# Amount: positive\n",
    "invalid_amounts = df[df['amount'] <= 0]\n",
    "validation_results['amount'] = len(invalid_amounts)\n",
    "print(f\"  amount (>0): {len(invalid_amounts)} invalid values\")\n",
    "\n",
    "# Velocity: non-negative\n",
    "invalid_velocity = df[df['velocity_last_24h'] < 0]\n",
    "validation_results['velocity'] = len(invalid_velocity)\n",
    "print(f\"  velocity_last_24h (≥0): {len(invalid_velocity)} invalid values\")\n",
    "\n",
    "# Age: reasonable range\n",
    "invalid_age = df[(df['cardholder_age'] < 18) | (df['cardholder_age'] > 100)]\n",
    "validation_results['cardholder_age'] = len(invalid_age)\n",
    "print(f\"  cardholder_age (18-100): {len(invalid_age)} invalid values\")\n",
    "\n",
    "# Binary fields: 0 or 1\n",
    "binary_fields = ['foreign_transaction', 'location_mismatch', 'is_fraud']\n",
    "for field in binary_fields:\n",
    "    invalid_binary = df[~df[field].isin([0, 1])]\n",
    "    validation_results[field] = len(invalid_binary)\n",
    "    print(f\"  {field} (0/1): {len(invalid_binary)} invalid values\")\n",
    "\n",
    "total_invalid = sum(validation_results.values())\n",
    "if total_invalid == 0:\n",
    "    print(\"\\n  ✓ All domain constraints satisfied\")\n",
    "else:\n",
    "    print(f\"\\n  ⚠ Total domain violations: {total_invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] OUTLIER DETECTION\n",
      "--------------------------------------------------------------------------------\n",
      "           Feature  IQR_Outliers  Z_Score_Outliers  Lower_Bound  Upper_Bound  Min     Max\n",
      "            amount           501               180    -236.4575     529.8425  0.0 1471.04\n",
      "  transaction_hour             0                 0     -12.0000      36.0000  0.0   23.00\n",
      "device_trust_score             0                 0     -12.5000     135.5000 25.0   99.00\n",
      " velocity_last_24h            51                51      -2.0000       6.0000  0.0    9.00\n",
      "    cardholder_age             0                 0      -9.0000      95.0000 18.0   69.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 5] OUTLIER DETECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                     'velocity_last_24h', 'cardholder_age']\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for feature in numerical_features:\n",
    "    # IQR Method\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    \n",
    "    # Z-Score Method (threshold = 3)\n",
    "    z_scores = np.abs(stats.zscore(df[feature]))\n",
    "    z_outliers = df[z_scores > 3]\n",
    "    \n",
    "    outlier_summary.append({\n",
    "        'Feature': feature,\n",
    "        'IQR_Outliers': len(iqr_outliers),\n",
    "        'Z_Score_Outliers': len(z_outliers),\n",
    "        'Lower_Bound': lower_bound,\n",
    "        'Upper_Bound': upper_bound,\n",
    "        'Min': df[feature].min(),\n",
    "        'Max': df[feature].max()\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Statistical Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] STATISTICAL PROFILING\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Numerical Features Profile:\n",
      "                      count     mean      std   min     1%      5%     25%      50%     75%      95%      99%      max  skewness  kurtosis     cv\n",
      "amount              10000.0  175.950  175.393   0.0   1.94   9.329  50.905  122.095  242.48  530.208  796.323  1471.04     1.919     5.118  0.997\n",
      "transaction_hour    10000.0   11.593    6.923   0.0   0.00   1.000   6.000   12.000   18.00   22.000   23.000    23.00    -0.027    -1.206  0.597\n",
      "device_trust_score  10000.0   61.799   21.487  25.0  25.00  28.000  43.000   62.000   80.00   96.000   99.000    99.00     0.011    -1.180  0.348\n",
      "velocity_last_24h   10000.0    2.009    1.433   0.0   0.00   0.000   1.000    2.000    3.00    5.000    6.000     9.00     0.708     0.446  0.713\n",
      "cardholder_age      10000.0   43.469   14.979  18.0  18.00  20.000  30.000   44.000   56.00   67.000   69.000    69.00     0.004    -1.197  0.345\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 6] STATISTICAL PROFILING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                     'velocity_last_24h', 'cardholder_age']\n",
    "\n",
    "stats_profile = df[numerical_features].describe(\n",
    "    percentiles=[.01, .05, .25, .5, .75, .95, .99]\n",
    ").T\n",
    "\n",
    "# Add additional statistics\n",
    "stats_profile['skewness'] = df[numerical_features].skew()\n",
    "stats_profile['kurtosis'] = df[numerical_features].kurtosis()\n",
    "stats_profile['cv'] = stats_profile['std'] / stats_profile['mean']\n",
    "\n",
    "print(\"\\nNumerical Features Profile:\")\n",
    "print(stats_profile.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] TARGET VARIABLE ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Target Variable Distribution:\n",
      "  Normal (0): 9849 (98.49%)\n",
      "  Fraud (1):  151 (1.51%)\n",
      "\n",
      "  Imbalance Ratio: 65.23:1\n",
      "  ⚠ HIGH IMBALANCE - Consider resampling techniques\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 7] TARGET VARIABLE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "fraud_dist = df['is_fraud'].value_counts().sort_index()\n",
    "fraud_pct = df['is_fraud'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(f\"  Normal (0): {fraud_dist.get(0, 0)} ({fraud_pct.get(0, 0):.2f}%)\")\n",
    "print(f\"  Fraud (1):  {fraud_dist.get(1, 0)} ({fraud_pct.get(1, 0):.2f}%)\")\n",
    "\n",
    "if 1 in fraud_dist.index and 0 in fraud_dist.index:\n",
    "    imbalance_ratio = fraud_dist[0] / fraud_dist[1]\n",
    "    print(f\"\\n  Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(\"  ⚠ HIGH IMBALANCE - Consider resampling techniques\")\n",
    "    elif imbalance_ratio > 5:\n",
    "        print(\"  ⚠ MODERATE IMBALANCE - Monitor model performance\")\n",
    "    else:\n",
    "        print(\"  ✓ ACCEPTABLE BALANCE\")\n",
    "elif 1 not in fraud_dist.index:\n",
    "    print(\"\\n  ⚠ CRITICAL: No fraud cases in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Categorical Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 8] CATEGORICAL FEATURE ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Merchant Category Distribution:\n",
      "  Food           : 2093 (20.93%)\n",
      "  Clothing       : 2050 (20.50%)\n",
      "  Travel         : 1990 (19.90%)\n",
      "  Grocery        : 1944 (19.44%)\n",
      "  Electronics    : 1923 (19.23%)\n",
      "\n",
      "  Total unique categories: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 8] CATEGORICAL FEATURE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Merchant Category\n",
    "merchant_dist = df['merchant_category'].value_counts()\n",
    "merchant_pct = df['merchant_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nMerchant Category Distribution:\")\n",
    "for cat, count in merchant_dist.items():\n",
    "    print(f\"  {cat:15s}: {count:4d} ({merchant_pct[cat]:5.2f}%)\")\n",
    "\n",
    "print(f\"\\n  Total unique categories: {df['merchant_category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 9] CORRELATION ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Correlation with Target Variable (is_fraud):\n",
      "  foreign_transaction      :  0.1856\n",
      "  location_mismatch        :  0.1730\n",
      "  velocity_last_24h        :  0.1034\n",
      "  amount                   :  0.0284\n",
      "  cardholder_age           : -0.0006\n",
      "  device_trust_score       : -0.1379\n",
      "  transaction_hour         : -0.1387\n",
      "\n",
      "High Feature Correlations (|r| > 0.7):\n",
      "  ✓ No multicollinearity detected\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 9] CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "numerical_cols = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                 'velocity_last_24h', 'cardholder_age', \n",
    "                 'foreign_transaction', 'location_mismatch', 'is_fraud']\n",
    "\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "\n",
    "print(\"\\nCorrelation with Target Variable (is_fraud):\")\n",
    "target_corr = corr_matrix['is_fraud'].sort_values(ascending=False)\n",
    "for feature, corr_val in target_corr.items():\n",
    "    if feature != 'is_fraud':\n",
    "        print(f\"  {feature:25s}: {corr_val:7.4f}\")\n",
    "\n",
    "# High correlations between features (multicollinearity check)\n",
    "print(\"\\nHigh Feature Correlations (|r| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr_val in high_corr_pairs:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr_val:.4f}\")\n",
    "else:\n",
    "    print(\"  ✓ No multicollinearity detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA QUALITY SCORECARD\n",
      "================================================================================\n",
      "\n",
      "Dataset Size: 10000 records × 10 features\n",
      "\n",
      "Quality Dimensions:\n",
      "  ✓ Completeness   : 100.00%\n",
      "  ✓ Uniqueness     : 100.00%\n",
      "  ✓ Validity       :  95.00%\n",
      "\n",
      "========================================\n",
      "  Overall Quality Score: 98.33%\n",
      "========================================\n",
      "\n",
      "  Data Quality Grade: EXCELLENT\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY SCORECARD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_records = len(df)\n",
    "total_features = len(df.columns) - 1  # Excluding target\n",
    "\n",
    "# Calculate quality score\n",
    "missing_values = df.isnull().sum().sum()\n",
    "duplicates = df.duplicated().sum()\n",
    "domain_violations = sum(validation_results.values())\n",
    "\n",
    "quality_scores = {\n",
    "    'Completeness': 100 if missing_values == 0 else \n",
    "                  (1 - missing_values / (total_records * total_features)) * 100,\n",
    "    'Uniqueness': 100 if duplicates == 0 else\n",
    "                 (1 - duplicates / total_records) * 100,\n",
    "    'Validity': 100 if domain_violations == 0 else 95,\n",
    "}\n",
    "\n",
    "overall_quality = np.mean(list(quality_scores.values()))\n",
    "\n",
    "print(f\"\\nDataset Size: {total_records} records × {total_features + 1} features\")\n",
    "print(f\"\\nQuality Dimensions:\")\n",
    "for dimension, score in quality_scores.items():\n",
    "    status = \"✓\" if score >= 95 else \"⚠\"\n",
    "    print(f\"  {status} {dimension:15s}: {score:6.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"  Overall Quality Score: {overall_quality:.2f}%\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# Quality grade\n",
    "if overall_quality >= 95:\n",
    "    grade = \"EXCELLENT\"\n",
    "elif overall_quality >= 85:\n",
    "    grade = \"GOOD\"\n",
    "elif overall_quality >= 70:\n",
    "    grade = \"FAIR\"\n",
    "else:\n",
    "    grade = \"POOR\"\n",
    "\n",
    "print(f\"\\n  Data Quality Grade: {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UNIVARIATE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "AMOUNT\n",
      "----------------------------------------\n",
      "  Mean:       175.95\n",
      "  Median:     122.09\n",
      "  Std Dev:    175.39\n",
      "  Range:      [0.00, 1471.04]\n",
      "  IQR:        191.57\n",
      "  Skewness:   1.919 (Right-skewed)\n",
      "  Kurtosis:   5.118 (Heavy-tailed)\n",
      "\n",
      "TRANSACTION_HOUR\n",
      "----------------------------------------\n",
      "  Mean:       11.59\n",
      "  Median:     12.00\n",
      "  Std Dev:    6.92\n",
      "  Range:      [0.00, 23.00]\n",
      "  IQR:        12.00\n",
      "  Skewness:   -0.027 (Left-skewed)\n",
      "  Kurtosis:   -1.206 (Light-tailed)\n",
      "\n",
      "DEVICE_TRUST_SCORE\n",
      "----------------------------------------\n",
      "  Mean:       61.80\n",
      "  Median:     62.00\n",
      "  Std Dev:    21.49\n",
      "  Range:      [25.00, 99.00]\n",
      "  IQR:        37.00\n",
      "  Skewness:   0.011 (Right-skewed)\n",
      "  Kurtosis:   -1.180 (Light-tailed)\n",
      "\n",
      "VELOCITY_LAST_24H\n",
      "----------------------------------------\n",
      "  Mean:       2.01\n",
      "  Median:     2.00\n",
      "  Std Dev:    1.43\n",
      "  Range:      [0.00, 9.00]\n",
      "  IQR:        2.00\n",
      "  Skewness:   0.708 (Right-skewed)\n",
      "  Kurtosis:   0.446 (Heavy-tailed)\n",
      "\n",
      "CARDHOLDER_AGE\n",
      "----------------------------------------\n",
      "  Mean:       43.47\n",
      "  Median:     44.00\n",
      "  Std Dev:    14.98\n",
      "  Range:      [18.00, 69.00]\n",
      "  IQR:        26.00\n",
      "  Skewness:   0.004 (Right-skewed)\n",
      "  Kurtosis:   -1.197 (Light-tailed)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNIVARIATE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                     'velocity_last_24h', 'cardholder_age']\n",
    "\n",
    "for feature in numerical_features:\n",
    "    print(f\"\\n{feature.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Descriptive statistics\n",
    "    print(f\"  Mean:       {df[feature].mean():.2f}\")\n",
    "    print(f\"  Median:     {df[feature].median():.2f}\")\n",
    "    print(f\"  Std Dev:    {df[feature].std():.2f}\")\n",
    "    print(f\"  Range:      [{df[feature].min():.2f}, {df[feature].max():.2f}]\")\n",
    "    print(f\"  IQR:        {df[feature].quantile(0.75) - df[feature].quantile(0.25):.2f}\")\n",
    "    \n",
    "    # Distribution shape\n",
    "    skew = df[feature].skew()\n",
    "    kurt = df[feature].kurtosis()\n",
    "    print(f\"  Skewness:   {skew:.3f} {'(Right-skewed)' if skew > 0 else '(Left-skewed)' if skew < 0 else '(Symmetric)'}\")\n",
    "    print(f\"  Kurtosis:   {kurt:.3f} {'(Heavy-tailed)' if kurt > 0 else '(Light-tailed)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BIVARIATE ANALYSIS (Features vs Fraud)\n",
      "================================================================================\n",
      "\n",
      "AMOUNT by Fraud Status:\n",
      "----------------------------------------\n",
      "           count        mean         std   min    25%     50%      75%      max\n",
      "is_fraud                                                                       \n",
      "0         9849.0  175.333015  173.986837  0.00  50.99  122.11  241.650  1471.04\n",
      "1          151.0  216.182980  248.120467  0.11  41.53  118.94  341.695  1185.07\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.5720\n",
      "  ✗ No significant difference\n",
      "\n",
      "TRANSACTION_HOUR by Fraud Status:\n",
      "----------------------------------------\n",
      "           count       mean       std  min  25%   50%   75%   max\n",
      "is_fraud                                                         \n",
      "0         9849.0  11.712154  6.870960  0.0  6.0  12.0  18.0  23.0\n",
      "1          151.0   3.841060  5.803554  0.0  1.0   2.0   3.0  23.0\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.0000\n",
      "  ✓ Significant difference between groups\n",
      "\n",
      "DEVICE_TRUST_SCORE by Fraud Status:\n",
      "----------------------------------------\n",
      "           count       mean        std   min   25%   50%   75%   max\n",
      "is_fraud                                                            \n",
      "0         9849.0  62.165804  21.351099  25.0  44.0  62.0  80.0  99.0\n",
      "1          151.0  37.867550  16.179277  25.0  28.5  32.0  38.0  99.0\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.0000\n",
      "  ✓ Significant difference between groups\n",
      "\n",
      "VELOCITY_LAST_24H by Fraud Status:\n",
      "----------------------------------------\n",
      "           count      mean       std  min  25%  50%  75%  max\n",
      "is_fraud                                                     \n",
      "0         9849.0  1.990557  1.415366  0.0  1.0  2.0  3.0  9.0\n",
      "1          151.0  3.205298  1.953861  0.0  2.0  3.0  5.0  7.0\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.0000\n",
      "  ✓ Significant difference between groups\n",
      "\n",
      "CARDHOLDER_AGE by Fraud Status:\n",
      "----------------------------------------\n",
      "           count       mean        std   min   25%   50%   75%   max\n",
      "is_fraud                                                            \n",
      "0         9849.0  43.469794  14.987218  18.0  30.0  44.0  56.0  69.0\n",
      "1          151.0  43.397351  14.490953  18.0  31.5  44.0  55.5  69.0\n",
      "\n",
      "  Mann-Whitney U test p-value: 0.9558\n",
      "  ✗ No significant difference\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BIVARIATE ANALYSIS (Features vs Fraud)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if df['is_fraud'].nunique() < 2:\n",
    "    print(\"\\n  ⚠ Cannot perform bivariate analysis - only one class present\")\n",
    "else:\n",
    "    numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                         'velocity_last_24h', 'cardholder_age']\n",
    "    \n",
    "    for feature in numerical_features:\n",
    "        print(f\"\\n{feature.upper()} by Fraud Status:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        fraud_stats = df.groupby('is_fraud')[feature].describe()\n",
    "        print(fraud_stats)\n",
    "        \n",
    "        # Statistical test (Mann-Whitney U)\n",
    "        normal_vals = df[df['is_fraud'] == 0][feature]\n",
    "        fraud_vals = df[df['is_fraud'] == 1][feature]\n",
    "        \n",
    "        if len(fraud_vals) > 0:\n",
    "            stat, p_value = mannwhitneyu(normal_vals, fraud_vals)\n",
    "            print(f\"\\n  Mann-Whitney U test p-value: {p_value:.4f}\")\n",
    "            if p_value < 0.05:\n",
    "                print(f\"  ✓ Significant difference between groups\")\n",
    "            else:\n",
    "                print(f\"  ✗ No significant difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTIVARIATE PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Average Amount by Merchant Category:\n",
      "                     mean     std  count\n",
      "merchant_category                       \n",
      "Clothing           176.02  175.64   2050\n",
      "Electronics        178.61  183.51   1923\n",
      "Food               173.83  169.03   2093\n",
      "Grocery            176.72  177.00   1944\n",
      "Travel             174.79  172.21   1990\n",
      "\n",
      "Risk Factor Combinations:\n",
      "                                      is_fraud            amount device_trust_score\n",
      "                                         count sum  mean    mean               mean\n",
      "foreign_transaction location_mismatch                                              \n",
      "0                   0                     8250  26  0.00  175.58              61.67\n",
      "                    1                      772  43  0.06  177.12              62.11\n",
      "1                   0                      893  53  0.06  177.02              62.83\n",
      "                    1                       85  29  0.34  189.79              60.53\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTIVARIATE PATTERN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Amount by merchant and fraud\n",
    "print(\"\\nAverage Amount by Merchant Category:\")\n",
    "merchant_amount = df.groupby('merchant_category')['amount'].agg(['mean', 'std', 'count'])\n",
    "print(merchant_amount.round(2))\n",
    "\n",
    "# Risk factor combinations\n",
    "print(\"\\nRisk Factor Combinations:\")\n",
    "risk_analysis = df.groupby(['foreign_transaction', 'location_mismatch']).agg({\n",
    "    'is_fraud': ['count', 'sum', 'mean'],\n",
    "    'amount': 'mean',\n",
    "    'device_trust_score': 'mean'\n",
    "}).round(2)\n",
    "print(risk_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE INDICATORS\n",
      "================================================================================\n",
      "\n",
      "Feature Variance (Higher = More Informative):\n",
      "  amount                   : Variance=  30762.64, CV=0.9968\n",
      "  transaction_hour         : Variance=     47.92, CV=0.5971\n",
      "  device_trust_score       : Variance=    461.69, CV=0.3477\n",
      "  velocity_last_24h        : Variance=      2.05, CV=0.7131\n",
      "  cardholder_age           : Variance=    224.37, CV=0.3446\n",
      "\n",
      "Preliminary Feature-Target Association:\n",
      "  amount                   : r= 0.0284\n",
      "  transaction_hour         : r=-0.1387\n",
      "  device_trust_score       : r=-0.1379\n",
      "  velocity_last_24h        : r= 0.1034\n",
      "  cardholder_age           : r=-0.0006\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE INDICATORS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "numerical_features = ['amount', 'transaction_hour', 'device_trust_score', \n",
    "                     'velocity_last_24h', 'cardholder_age']\n",
    "\n",
    "# Variance-based\n",
    "print(\"\\nFeature Variance (Higher = More Informative):\")\n",
    "for feature in numerical_features:\n",
    "    var = df[feature].var()\n",
    "    cv = df[feature].std() / df[feature].mean() if df[feature].mean() != 0 else 0\n",
    "    print(f\"  {feature:25s}: Variance={var:10.2f}, CV={cv:.4f}\")\n",
    "\n",
    "# Information value (if fraud cases exist)\n",
    "if df['is_fraud'].sum() > 0:\n",
    "    print(\"\\nPreliminary Feature-Target Association:\")\n",
    "    for feature in numerical_features:\n",
    "        corr = df[feature].corr(df['is_fraud'])\n",
    "        print(f\"  {feature:25s}: r={corr:7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Executive Summary & Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXECUTIVE SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Data Cleaning Completed\n",
      "✓ Exploratory Data Analysis Completed\n",
      "✓ Data Quality Score: 98.33%\n",
      "✓ Ready for Feature Engineering & Modeling\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✓ Cleaned data saved to: Cleaned_Data/credit_card_fraud_10k_cleaned.csv\n",
      "✓ Total records saved: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n✓ Data Cleaning Completed\")\n",
    "print(f\"✓ Exploratory Data Analysis Completed\")\n",
    "print(f\"✓ Data Quality Score: {overall_quality:.2f}%\")\n",
    "print(f\"✓ Ready for Feature Engineering & Modeling\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv(output_filepath, index=False)\n",
    "print(f\"\\n✓ Cleaned data saved to: {output_filepath}\")\n",
    "print(f\"✓ Total records saved: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_hour</th>\n",
       "      <th>merchant_category</th>\n",
       "      <th>foreign_transaction</th>\n",
       "      <th>location_mismatch</th>\n",
       "      <th>device_trust_score</th>\n",
       "      <th>velocity_last_24h</th>\n",
       "      <th>cardholder_age</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>84.47</td>\n",
       "      <td>22</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>541.82</td>\n",
       "      <td>3</td>\n",
       "      <td>Travel</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>237.01</td>\n",
       "      <td>17</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>164.33</td>\n",
       "      <td>4</td>\n",
       "      <td>Grocery</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>30.53</td>\n",
       "      <td>15</td>\n",
       "      <td>Food</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id  amount  transaction_hour merchant_category  foreign_transaction  location_mismatch  device_trust_score  velocity_last_24h  cardholder_age  is_fraud\n",
       "0               1   84.47                22       Electronics                    0                  0                  66                  3              40         0\n",
       "1               2  541.82                 3            Travel                    1                  0                  87                  1              64         0\n",
       "2               3  237.01                17           Grocery                    0                  0                  49                  1              61         0\n",
       "3               4  164.33                 4           Grocery                    0                  1                  72                  3              34         0\n",
       "4               5   30.53                15              Food                    0                  0                  79                  0              44         0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of cleaned data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   transaction_id       10000 non-null  int64  \n",
      " 1   amount               10000 non-null  float64\n",
      " 2   transaction_hour     10000 non-null  int64  \n",
      " 3   merchant_category    10000 non-null  object \n",
      " 4   foreign_transaction  10000 non-null  int64  \n",
      " 5   location_mismatch    10000 non-null  int64  \n",
      " 6   device_trust_score   10000 non-null  int64  \n",
      " 7   velocity_last_24h    10000 non-null  int64  \n",
      " 8   cardholder_age       10000 non-null  int64  \n",
      " 9   is_fraud             10000 non-null  int64  \n",
      "dtypes: float64(1), int64(8), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display basic info\n",
    "print(\"\\nDataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_hour</th>\n",
       "      <th>foreign_transaction</th>\n",
       "      <th>location_mismatch</th>\n",
       "      <th>device_trust_score</th>\n",
       "      <th>velocity_last_24h</th>\n",
       "      <th>cardholder_age</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>175.949849</td>\n",
       "      <td>11.593300</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.085700</td>\n",
       "      <td>61.798900</td>\n",
       "      <td>2.008900</td>\n",
       "      <td>43.468700</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>175.392827</td>\n",
       "      <td>6.922708</td>\n",
       "      <td>0.297059</td>\n",
       "      <td>0.279935</td>\n",
       "      <td>21.487053</td>\n",
       "      <td>1.432559</td>\n",
       "      <td>14.979147</td>\n",
       "      <td>0.121957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>50.905000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>122.095000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>242.480000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1471.040000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       transaction_id        amount  transaction_hour  foreign_transaction  location_mismatch  device_trust_score  velocity_last_24h  cardholder_age      is_fraud\n",
       "count     10000.00000  10000.000000      10000.000000         10000.000000       10000.000000        10000.000000       10000.000000    10000.000000  10000.000000\n",
       "mean       5000.50000    175.949849         11.593300             0.097800           0.085700           61.798900           2.008900       43.468700      0.015100\n",
       "std        2886.89568    175.392827          6.922708             0.297059           0.279935           21.487053           1.432559       14.979147      0.121957\n",
       "min           1.00000      0.000000          0.000000             0.000000           0.000000           25.000000           0.000000       18.000000      0.000000\n",
       "25%        2500.75000     50.905000          6.000000             0.000000           0.000000           43.000000           1.000000       30.000000      0.000000\n",
       "50%        5000.50000    122.095000         12.000000             0.000000           0.000000           62.000000           2.000000       44.000000      0.000000\n",
       "75%        7500.25000    242.480000         18.000000             0.000000           0.000000           80.000000           3.000000       56.000000      0.000000\n",
       "max       10000.00000   1471.040000         23.000000             1.000000           1.000000           99.000000           9.000000       69.000000      1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
